{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mani1213/LLM-s/blob/main/desc2titles_llama_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6240238",
      "metadata": {
        "id": "f6240238",
        "outputId": "2ccc9581-9e30-4c82-dd7d-2cfb86882965"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-25 07:54:59.101914: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-10-25 07:54:59.170009: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-25 07:54:59.170057: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-25 07:54:59.170225: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-25 07:54:59.191005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-25 07:55:00.143904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset,DatasetDict,Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8960b50",
      "metadata": {
        "id": "d8960b50"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"desc2title_data.parquet\")\n",
        "df = df.sample(100000,random_state=45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7995ccf",
      "metadata": {
        "id": "a7995ccf"
      },
      "outputs": [],
      "source": [
        "df=df[df.cleaned_title.apply(lambda x:isinstance(x,str))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c710d5",
      "metadata": {
        "id": "e9c710d5",
        "outputId": "e3e28a0c-4a80-476f-d3e0-88a9581b24f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_279411/1108369352.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text']=df.apply(lambda x:\"<s>[INST] Generate multiple relevant jobtitles for the description below ?\\n\"+x['description']+\"[/INST] \"+x['cleaned_title']+\" </s>\",axis=1)\n"
          ]
        }
      ],
      "source": [
        "df['text']=df.apply(lambda x:\"<s>[INST] Generate multiple relevant jobtitles for the description below ?\\n\"+x['description']+\"[/INST] \"+x['cleaned_title']+\" </s>\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe78617",
      "metadata": {
        "id": "2fe78617"
      },
      "outputs": [],
      "source": [
        "df=df[['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad838b3",
      "metadata": {
        "id": "2ad838b3"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = True\n",
        "\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 2\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = True\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 8\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 8\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 15\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd0d82e",
      "metadata": {
        "id": "1dd0d82e",
        "outputId": "13f7b113-4eb4-4965-b553-4e7b8a4192a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', '__index_level_0__'],\n",
              "    num_rows: 99997\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset=Dataset.from_pandas(df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605b4ee2",
      "metadata": {
        "id": "605b4ee2",
        "outputId": "85a74208-c399-4606-e7cb-d24c767dbbe6",
        "colab": {
          "referenced_widgets": [
            "8e3c8fc13507489fb14e40a30eaad8ae",
            "af08f5c41abb47c3916930afaa9e7b88"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e3c8fc13507489fb14e40a30eaad8ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mlteam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/mlteam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/mlteam/.local/lib/python3.10/site-packages/peft/utils/other.py:107: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af08f5c41abb47c3916930afaa9e7b88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99997 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/home/mlteam/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   66/25000 04:12 < 27:22:32, 0.25 it/s, Epoch 0.01/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.400300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78109274",
      "metadata": {
        "id": "78109274"
      },
      "outputs": [],
      "source": [
        "new_model=\"results/desc2title_llama\"\n",
        "trainer.model.save_pretrained(new_model)\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffeeb81d",
      "metadata": {
        "id": "ffeeb81d"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618c7b33",
      "metadata": {
        "id": "618c7b33",
        "outputId": "d7cec165-d2e6-4da9-d175-7f0088ab1fc0",
        "colab": {
          "referenced_widgets": [
            "ea9eff82690f4233bec5c117f977639e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea9eff82690f4233bec5c117f977639e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mlteam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/mlteam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "pipe= pipeline(task=\"text-generation\", model=\"results/desc2title_llama\", tokenizer=\"results/desc2title_llama\", max_length=350,use_fast=True,device_map = {\"\": 0},max_new_tokens=350,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa38fce3",
      "metadata": {
        "id": "fa38fce3"
      },
      "outputs": [],
      "source": [
        "prompt= \"Generate 30 relevant jobtitles for the description below ?\\n Warehouse ReceiverPay rate: Up to $18.40Job Summary!You will become part of the team at the Amazon warehouse that takes care of the large items&mdash;the items that customer orders and weigh over 50 pounds, like furniture, big-screen TVs, appliances, and more. You will get the necessary training to use the latest technology to move these heavy bulk products. Your non-experience in handling these items will not be a barrier to gaining valuable job inputs, and you will be training on powered industrial trucks (PIT) such as pallet drivers and forklifts. There will be shifts where you have to manage our power equipment to get the customer orders ready; at other times, you will be a part of our warehouse team at fulfillment centers or delivery stations handling those hands-on items that require expert service before they are sent for delivery. If you are someone who wants to become a heavy equipment operator, this is the opportunity for you.What are the duties and responsibilities of the job?The job duties include:You need to operate powered industrial trucks (PITs). Training on equipment and certification shall be given free of cost.Follow the on-screen prompts for certain tasks and work as per directions.Take count of the new bulk and heavy inventory and put them away.Load the trucks with order packages for shipment.Use technology like smartphones and computers for processing orders.Work and operate such machinery that reaches a height of 35 ft.Additional responsibilities would include:Lifting weights up to 49 pounds.During shifts, use hand trucks, dollies, carts, and other gears to move things around.Stand, walk, squat, push, pull, bend, and reach during your shifts.Going up and down the stairs wherever applicable.Follow the safety standards to handle products that weigh more than 50 pounds.What is the vibe at an Amazon warehouse?Noise: The place can get noisy because of different warehouse activities. You will get hearing protection gears from us if you need them.Surroundings: Your surrounding will be filled with stand-up forklifts, order pickers, turret trucks, moving machines, and mobile carts.Activity: Your job might make you stand in one place for a long time, climb stairs, or walk around.Dress code: There is no dress code. However, you will have to follow a few rules for safety measures. You will have to wear comfortable, closed-toe shoes. Based on the job location or the role, Amazon will provide you with a $110 Zappos gift code for purchasing shoes to prepare you for your first day on the job.Temperature: You will have to work in extreme temperatures. Though there are climate control measures inside the warehouse, temperature could vary between 60&deg;F and 90&deg;F in some parts of the warehouse. During summers, the temperature can exceed 90&deg;F inside trailers or the truck yard.Safety: Safety is a top priority for all of us. Every employee is provided with appropriate safety gear, and the teams ensure safety tips are shared on a regular basis.Why will you want to do this job?Team environment: At Amazon, the team environment is very supportive; team members support one another, even if it is a small warehouse or a big one.Shift flexibility: You can enjoy a flexible schedule. You can take your shifts in the early morning to shift in the daytime, evenings, or even overnight and weekend shifts. There are options for three-day shifts and four-day shifts. You can also opt for anytime shifts for as little as 4 hours a week.Schedule options:There are multiple work schedule options to choose from. Based on your preference, you can work full-time (40 hours a week), reduced time (30 to 36 hours a week), or part-time (20 hours or less). You also have the option of working overtime.Free training: You will learn how to drive power industrial trucks like pallet drivers or forklifts and how to use other advanced Amazon technologies, including working with robots.Career advancement: We as a company believe in upgrading our employee skills with various career development programs, including training and upscaling programs. We also provide tuition reimbursement to a select group of employees.Stay occupied: Such is your job that it will keep you and your team engaged throughout the shift hours.New skills: Based on your role and location, you will learn how to use the latest Amazon technologies, including robotics and handheld gadgets.Why is Amazon so preferred?Many of our jobs come with loads of benefits, including career development programs, health benefits, parental leaves, and ways to save for the future. We believe in an inclusive and safe working ecosystem. Amazon is considered one of the best places to work in the world. Our offerings may vary as per the job role, location, and work schedule. Some of the benefits include:401(k) savings option.Employee Assistance Program.Competitive wages are paid weekly.Healthcare includes medical, optical, dental benefits and prescribed drugsPaid time off (PTO).Holiday pay options.Skill development and training while on the job.Employee discounts on various items and restaurants.Minimum life insurance.AD&amp;D insurance.Company-supported long-term and short-term disability benefits.On-the-job training and skill enhancement.Employee Assistance Program.Maternity and parental leaves.Adoption assistance.Learn more about Amazon Benefits and Culturehttps://hiring.amazon.com/why-amazon/benefits#/https://hiring.amazon.com/why-amazon/culture#/Requirements:Candidates must be 18 years of age or older with the ability to acknowledge and comply with the job requirements and safety guidelines.How to apply?You can begin by applying here. We can help you with the application process, or learn more about how hiring happens.For details, visit https://hiring.amazon.com/hiring-process#/.If you have a disability and need accommodation during the application and hiring process, including support for the New Hire Event, or need to initiate a request prior to starting your Day 1, please visit https://hiring.amazon.com/people-with-disabilities#/ or contact the Applicant-Candidate Accommodation Team (ACAT). You can reach us by phone at 888-435-9287, Monday through Friday, between 6 a.m. and 4 p.m. PT.Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or another legally protected status.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f098dc98",
      "metadata": {
        "id": "f098dc98",
        "outputId": "17044d56-a0e9-435a-ca86-ea461fa11014"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=350) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 12.3 s, sys: 147 ms, total: 12.5 s\n",
            "Wall time: 12.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "des=pipe(f\"<s>[INST] {prompt} [/INST]\")[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774f90dc",
      "metadata": {
        "id": "774f90dc",
        "outputId": "689d027d-2145-4e35-c1c3-4f7a34e2d84d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1. Warehouse Receiver',\n",
              " '2. Inventory Specialist',\n",
              " '3. Bulk Receiver',\n",
              " '4. Heavy Equipment Operator',\n",
              " '5. Powered Industrial Truck Operator',\n",
              " '6. Forklift Driver',\n",
              " '7. Warehouse Worker',\n",
              " '8. Order Fulfillment Specialist',\n",
              " '9. Shipping and Receiving Clerk',\n",
              " '10. Inventory Control Specialist',\n",
              " '11. Supply Chain Coordinator',\n",
              " '12. Warehouse Technician',\n",
              " '13. Material Handler',\n",
              " '14. Equipment Operator',\n",
              " '15. Shipping Supervisor',\n",
              " '16. Receiving Manager',\n",
              " '17. Inventory Lead',\n",
              " '18. Warehouse Lead',\n",
              " '19. Logistics Coordinator',\n",
              " '20. Supply Chain Analyst',\n",
              " '21. Warehouse Coordinator',\n",
              " '22. Inventory Analyst',\n",
              " '23. Shipping and Receiving Supervisor',\n",
              " '24. Supply Chain Manager',\n",
              " '25. Warehouse Operations Manager',\n",
              " '26. Inventory Planner',\n",
              " '27. Logistics Coordinator',\n",
              " '28. Shipping Manager',\n",
              " '29. Receiving Manager',\n",
              " '30. Warehouse Manager']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx=des.index('[/INST]')+7\n",
        "des=des[idx:]\n",
        "des.split(\"\\n\")[2:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b428f310",
      "metadata": {
        "id": "b428f310"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama",
      "language": "python",
      "name": "llama"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}